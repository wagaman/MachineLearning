# tf learning rate configuration part
# base_lr: base learning rate
# lr_decay_rate: learning rate decay rate, it is only used if any decay policy is defined in tf model
base_lr=0.1
lr_decay_rate=0.95

# training batch size
batch_size=100
# test batch size, it is possible that user may not include test phase in their model, we then need to find way to add test phase in their model
test_batch_size=64

# path to where the data sets are located
train_data=
test_data=
val_data=

# The max steps/iterations will be ran
max_steps=1000
# The frequency to run test phase 
#test_interval=100

# tf optimizer configuration part
# Supported optimizer in tf: GradientDescent, Adadelta, Adagrad, AdagradDA, Momentum, Adam, Ftrl, ProximalGradientDescent, ProximalAdagrad, RMSProp

# Configurable parameters for different optimizers
# only one optimizer can be configured
#optimizer=GradientDescent

#optimizer=Adadelta
#opt_decay=0.95
#epsilon=1e-08

#optimizer=Adagrad
#accumulator=0.1

#optimizer=AdagradDA
#accumulator=0.1
#l1_regularization=0.0
#l2_regularization=0.0

optimizer=Momentum
momentum=0.8

#optimizer=Adam
#beta1=0.9
#beta2=0.999
#epsilon=e-08

#optimizer=Ftrl
#lr_power=-0.5
#accumulator=0.1
#l1_regularization=0.0
#l2_regularization=0.0

#optimizer=ProximalGradientDescent
#l1_regularization=0.0
#l2_regularization=0.0

#optimizer=ProximalAdagrad
#accumulator=0.1
#l1_regularization=0.0
#l2_regularization=0.0

#optimizer=RMSProp
#opt_decay=0.9
#momentum=0.0
#epsilon=1e-10

